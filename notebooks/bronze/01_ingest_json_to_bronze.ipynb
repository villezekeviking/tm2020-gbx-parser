{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer: Ingest JSON to Bronze Delta Tables\n",
    "\n",
    "## Purpose\n",
    "This notebook ingests TrackMania 2020 replay data from JSON files stored in the Bronze Lakehouse Files area and writes them to Bronze Delta tables with minimal transformation.\n",
    "\n",
    "## Inputs\n",
    "- **Source**: JSON files in `tm2020_bronze` Lakehouse Files (`/lakehouse/default/Files/replays/*.json`)\n",
    "- **Format**: One JSON file per replay, structure based on `tm_gbx` parser output\n",
    "- **Schema**: metadata (player, map, race info) + ghost_samples (telemetry)\n",
    "\n",
    "## Outputs\n",
    "- **Target**: `bronze_replays_raw` Delta table in `tm2020_bronze` Lakehouse\n",
    "- **Append mode**: Preserves all historical data\n",
    "- **Columns**: replay_id, ingestion_timestamp, metadata, ghost_samples, source_file\n",
    "\n",
    "## Processing Logic\n",
    "1. Connect to Bronze Lakehouse\n",
    "2. Read JSON files from Files area\n",
    "3. Add ingestion metadata (timestamp, source file)\n",
    "4. Generate unique replay_id from filename or hash\n",
    "5. Write to Delta table (append mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakehouse connection setup\n",
    "# TODO: Configure Lakehouse connection for 'tm2020_bronze'\n",
    "# In Microsoft Fabric, the default lakehouse is automatically attached\n",
    "# Ensure this notebook is configured to use tm2020_bronze as default lakehouse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, DoubleType\n",
    "\n",
    "# Initialize Spark session (pre-configured in Fabric)\n",
    "spark = SparkSession.builder.appName(\"Bronze_JSON_Ingestion\").getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "# TODO: Update paths based on actual Lakehouse Files structure\n",
    "input_path = \"/lakehouse/default/Files/replays/*.json\"  # Path to JSON files\n",
    "output_table = \"bronze_replays_raw\"  # Bronze Delta table name\n",
    "\n",
    "print(f\"Input path: {input_path}\")\n",
    "print(f\"Output table: {output_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for JSON files based on tm_gbx output structure\n",
    "# This schema matches the output from tm_gbx/models.py\n",
    "\n",
    "# Vec3 schema (position/velocity)\n",
    "vec3_schema = StructType([\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Ghost sample schema\n",
    "ghost_sample_schema = StructType([\n",
    "    StructField(\"time_ms\", IntegerType(), True),\n",
    "    StructField(\"position\", vec3_schema, True),\n",
    "    StructField(\"velocity\", vec3_schema, True),\n",
    "    StructField(\"speed\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Metadata schema\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"player_login\", StringType(), True),\n",
    "    StructField(\"player_nickname\", StringType(), True),\n",
    "    StructField(\"map_name\", StringType(), True),\n",
    "    StructField(\"map_uid\", StringType(), True),\n",
    "    StructField(\"map_author\", StringType(), True),\n",
    "    StructField(\"race_time_ms\", IntegerType(), True),\n",
    "    StructField(\"checkpoints\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"num_respawns\", IntegerType(), True),\n",
    "    StructField(\"game_version\", StringType(), True),\n",
    "    StructField(\"title_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Complete JSON schema\n",
    "json_schema = StructType([\n",
    "    StructField(\"metadata\", metadata_schema, True),\n",
    "    StructField(\"ghost_samples\", ArrayType(ghost_sample_schema), True)\n",
    "])\n",
    "\n",
    "print(\"Schema defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON files from Lakehouse Files\n",
    "# TODO: Implement actual file reading logic\n",
    "# This is a placeholder - update based on actual file availability\n",
    "\n",
    "try:\n",
    "    # Read JSON files with schema\n",
    "    df_json = spark.read.schema(json_schema).json(input_path)\n",
    "    \n",
    "    # Add ingestion metadata\n",
    "    df_bronze = df_json \\\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"source_file\", input_file_name()) \\\n",
    "        .withColumn(\"replay_id\", lit(\"TODO: Generate from filename or hash\"))\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"Records read: {df_bronze.count()}\")\n",
    "    df_bronze.select(\"replay_id\", \"ingestion_timestamp\", \"source_file\", \"metadata.player_nickname\", \"metadata.race_time_ms\").show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON files: {e}\")\n",
    "    print(\"This is expected if no JSON files exist yet in the Lakehouse Files area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Bronze Delta table\n",
    "# TODO: Implement actual write logic after JSON files are available\n",
    "\n",
    "try:\n",
    "    # Write to Delta table in append mode\n",
    "    df_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(output_table)\n",
    "    \n",
    "    print(f\"Successfully wrote data to {output_table}\")\n",
    "    \n",
    "    # Verify write\n",
    "    df_verify = spark.table(output_table)\n",
    "    print(f\"Total records in {output_table}: {df_verify.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing to Delta table: {e}\")\n",
    "    print(\"Ensure df_bronze is defined (run previous cell successfully first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks (basic)\n",
    "# TODO: Implement data quality validation\n",
    "\n",
    "try:\n",
    "    df_check = spark.table(output_table)\n",
    "    \n",
    "    print(\"=== Data Quality Report ===\")\n",
    "    print(f\"Total records: {df_check.count()}\")\n",
    "    print(f\"Null metadata: {df_check.filter('metadata IS NULL').count()}\")\n",
    "    print(f\"Null ghost_samples: {df_check.filter('ghost_samples IS NULL').count()}\")\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nTable Schema:\")\n",
    "    df_check.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running data quality checks: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
