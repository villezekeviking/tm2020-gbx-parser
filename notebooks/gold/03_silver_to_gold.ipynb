{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer: Transform Silver to Gold Analytics Tables\n",
    "\n",
    "## Purpose\n",
    "This notebook creates business-ready aggregated datasets from Silver layer tables for analytics and reporting.\n",
    "\n",
    "## Inputs\n",
    "- **Source Lakehouse**: `tm2020_silver`\n",
    "- **Source Tables**:\n",
    "  - `silver_replays`: Replay metadata\n",
    "  - `silver_ghost_samples`: Telemetry samples\n",
    "  - `silver_maps`: Map dimension\n",
    "  - `silver_players`: Player dimension\n",
    "\n",
    "## Outputs\n",
    "- **Target Lakehouse**: `tm2020_gold`\n",
    "- **Tables**:\n",
    "  - `gold_player_stats`: Aggregate player performance metrics\n",
    "  - `gold_map_leaderboard`: Best times per map\n",
    "  - `gold_race_analytics`: Race performance analysis\n",
    "  - `gold_checkpoint_analysis`: Checkpoint-level insights\n",
    "\n",
    "## Business Metrics\n",
    "- Player statistics: total races, average times, best times, consistency\n",
    "- Leaderboards: rankings per map, global rankings\n",
    "- Race analytics: speed profiles, checkpoint performance\n",
    "- Trend analysis: performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session and imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, min, max, avg, sum as spark_sum, stddev,\n",
    "    row_number, rank, dense_rank, collect_list,\n",
    "    current_timestamp, to_date\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session (pre-configured in Fabric)\n",
    "spark = SparkSession.builder.appName(\"Gold_Aggregation\").getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table names\n",
    "# TODO: Ensure both Silver and Gold Lakehouses are attached to this notebook\n",
    "silver_replays = \"tm2020_silver.silver_replays\"\n",
    "silver_ghost_samples = \"tm2020_silver.silver_ghost_samples\"\n",
    "silver_maps = \"tm2020_silver.silver_maps\"\n",
    "silver_players = \"tm2020_silver.silver_players\"\n",
    "\n",
    "gold_player_stats = \"gold_player_stats\"\n",
    "gold_map_leaderboard = \"gold_map_leaderboard\"\n",
    "gold_race_analytics = \"gold_race_analytics\"\n",
    "gold_checkpoint_analysis = \"gold_checkpoint_analysis\"\n",
    "\n",
    "print(\"Silver sources configured\")\n",
    "print(\"Gold targets configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Silver layer\n",
    "# TODO: Implement incremental processing for Gold layer\n",
    "\n",
    "try:\n",
    "    df_replays = spark.table(silver_replays)\n",
    "    df_ghost_samples = spark.table(silver_ghost_samples)\n",
    "    df_maps = spark.table(silver_maps)\n",
    "    df_players = spark.table(silver_players)\n",
    "    \n",
    "    print(f\"Replays: {df_replays.count()} records\")\n",
    "    print(f\"Ghost samples: {df_ghost_samples.count()} records\")\n",
    "    print(f\"Maps: {df_maps.count()} records\")\n",
    "    print(f\"Players: {df_players.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Silver tables: {e}\")\n",
    "    print(\"Ensure Silver transformation notebook has been run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold Player Statistics table\n",
    "# TODO: Add more advanced metrics (consistency, improvement trends)\n",
    "\n",
    "try:\n",
    "    df_gold_player_stats = df_replays.groupBy(\"player_login\").agg(\n",
    "        count(\"replay_id\").alias(\"total_races\"),\n",
    "        count(col(\"map_uid\").isNotNull()).alias(\"total_maps_played\"),\n",
    "        avg(\"race_time_ms\").alias(\"avg_race_time_ms\"),\n",
    "        min(\"race_time_ms\").alias(\"best_race_time_ms\"),\n",
    "        spark_sum(\"num_respawns\").alias(\"total_respawns\"),\n",
    "        max(\"ingestion_date\").alias(\"last_race_date\")\n",
    "    )\n",
    "    \n",
    "    # Join with player dimension for nickname\n",
    "    df_gold_player_stats = df_gold_player_stats.join(\n",
    "        df_players.select(\"player_login\", \"player_nickname\"),\n",
    "        \"player_login\",\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        \"player_login\",\n",
    "        \"player_nickname\",\n",
    "        \"total_races\",\n",
    "        \"total_maps_played\",\n",
    "        \"avg_race_time_ms\",\n",
    "        \"best_race_time_ms\",\n",
    "        \"total_respawns\",\n",
    "        \"last_race_date\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Gold player stats: {df_gold_player_stats.count()} records\")\n",
    "    df_gold_player_stats.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating gold_player_stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold Map Leaderboard table\n",
    "# TODO: Handle ties in rankings, add percentile rankings\n",
    "\n",
    "try:\n",
    "    # Get best time per player per map\n",
    "    df_best_times = df_replays.groupBy(\"player_login\", \"map_uid\").agg(\n",
    "        min(\"race_time_ms\").alias(\"best_time_ms\"),\n",
    "        max(\"ingestion_date\").alias(\"race_date\")\n",
    "    )\n",
    "    \n",
    "    # Add rankings per map\n",
    "    window_spec = Window.partitionBy(\"map_uid\").orderBy(col(\"best_time_ms\").asc())\n",
    "    df_gold_leaderboard = df_best_times.withColumn(\n",
    "        \"rank\",\n",
    "        rank().over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # Join with map and player dimensions\n",
    "    df_gold_leaderboard = df_gold_leaderboard \\\n",
    "        .join(df_maps.select(\"map_uid\", \"map_name\"), \"map_uid\", \"left\") \\\n",
    "        .join(df_players.select(\"player_login\", \"player_nickname\"), \"player_login\", \"left\") \\\n",
    "        .select(\n",
    "            \"map_uid\",\n",
    "            \"map_name\",\n",
    "            \"player_login\",\n",
    "            \"player_nickname\",\n",
    "            \"best_time_ms\",\n",
    "            \"rank\",\n",
    "            \"race_date\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Gold map leaderboard: {df_gold_leaderboard.count()} records\")\n",
    "    df_gold_leaderboard.filter(col(\"rank\") <= 10).show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating gold_map_leaderboard: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold Race Analytics table\n",
    "# TODO: Add more telemetry-based metrics (trajectory analysis, cornering speed)\n",
    "\n",
    "try:\n",
    "    # Calculate speed statistics per replay from ghost samples\n",
    "    df_speed_stats = df_ghost_samples.groupBy(\"replay_id\").agg(\n",
    "        avg(\"speed\").alias(\"avg_speed\"),\n",
    "        max(\"speed\").alias(\"max_speed\"),\n",
    "        stddev(\"speed\").alias(\"speed_stddev\")\n",
    "    )\n",
    "    \n",
    "    # Join with replay metadata\n",
    "    df_gold_race_analytics = df_replays \\\n",
    "        .join(df_speed_stats, \"replay_id\", \"left\") \\\n",
    "        .select(\n",
    "            \"replay_id\",\n",
    "            \"player_login\",\n",
    "            \"map_uid\",\n",
    "            \"race_time_ms\",\n",
    "            \"num_respawns\",\n",
    "            \"avg_speed\",\n",
    "            \"max_speed\",\n",
    "            \"speed_stddev\",\n",
    "            \"ingestion_date\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Gold race analytics: {df_gold_race_analytics.count()} records\")\n",
    "    df_gold_race_analytics.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating gold_race_analytics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold Checkpoint Analysis table (placeholder)\n",
    "# TODO: Extract checkpoint times from metadata and analyze\n",
    "# This requires parsing the checkpoints array from replays\n",
    "\n",
    "try:\n",
    "    # This is a placeholder - actual implementation requires\n",
    "    # extracting checkpoint arrays and analyzing them\n",
    "    # For now, create basic statistics from available data\n",
    "    \n",
    "    df_gold_checkpoint = df_replays.groupBy(\"map_uid\").agg(\n",
    "        count(\"replay_id\").alias(\"total_races\"),\n",
    "        avg(\"num_checkpoints\").alias(\"avg_checkpoints\"),\n",
    "        avg(\"race_time_ms\").alias(\"avg_race_time_ms\")\n",
    "    ).join(\n",
    "        df_maps.select(\"map_uid\", \"map_name\"),\n",
    "        \"map_uid\",\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        \"map_uid\",\n",
    "        \"map_name\",\n",
    "        \"total_races\",\n",
    "        \"avg_checkpoints\",\n",
    "        \"avg_race_time_ms\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Gold checkpoint analysis: {df_gold_checkpoint.count()} records\")\n",
    "    df_gold_checkpoint.show(5, truncate=False)\n",
    "    \n",
    "    print(\"\\nNote: Full checkpoint analysis requires extracting checkpoint times from metadata.checkpoints array\")\n",
    "    print(\"This is a placeholder showing basic map-level statistics\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating gold_checkpoint_analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Gold Delta tables\n",
    "# TODO: Implement incremental refresh strategy\n",
    "# For now using overwrite mode - optimize later\n",
    "\n",
    "try:\n",
    "    # Write gold_player_stats\n",
    "    df_gold_player_stats.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(gold_player_stats)\n",
    "    print(f\"Successfully wrote to {gold_player_stats}\")\n",
    "    \n",
    "    # Write gold_map_leaderboard\n",
    "    df_gold_leaderboard.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(gold_map_leaderboard)\n",
    "    print(f\"Successfully wrote to {gold_map_leaderboard}\")\n",
    "    \n",
    "    # Write gold_race_analytics\n",
    "    df_gold_race_analytics.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(gold_race_analytics)\n",
    "    print(f\"Successfully wrote to {gold_race_analytics}\")\n",
    "    \n",
    "    # Write gold_checkpoint_analysis\n",
    "    df_gold_checkpoint.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(gold_checkpoint_analysis)\n",
    "    print(f\"Successfully wrote to {gold_checkpoint_analysis}\")\n",
    "    \n",
    "    print(\"\\nAll Gold tables written successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing Gold tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold layer summary report\n",
    "# TODO: Create comprehensive analytics summary\n",
    "\n",
    "try:\n",
    "    print(\"=== Gold Layer Summary Report ===\")\n",
    "    \n",
    "    # Player stats summary\n",
    "    df_player_stats_check = spark.table(gold_player_stats)\n",
    "    print(f\"\\ngold_player_stats: {df_player_stats_check.count()} players\")\n",
    "    print(\"Top 5 players by total races:\")\n",
    "    df_player_stats_check.orderBy(col(\"total_races\").desc()).show(5, truncate=False)\n",
    "    \n",
    "    # Leaderboard summary\n",
    "    df_leaderboard_check = spark.table(gold_map_leaderboard)\n",
    "    print(f\"\\ngold_map_leaderboard: {df_leaderboard_check.count()} entries\")\n",
    "    print(\"Sample top 5 entries:\")\n",
    "    df_leaderboard_check.filter(col(\"rank\") <= 5).show(5, truncate=False)\n",
    "    \n",
    "    # Race analytics summary\n",
    "    df_race_analytics_check = spark.table(gold_race_analytics)\n",
    "    print(f\"\\ngold_race_analytics: {df_race_analytics_check.count()} races\")\n",
    "    print(\"Average statistics:\")\n",
    "    df_race_analytics_check.select(\n",
    "        avg(\"race_time_ms\").alias(\"avg_race_time\"),\n",
    "        avg(\"avg_speed\").alias(\"avg_speed\"),\n",
    "        avg(\"max_speed\").alias(\"avg_max_speed\")\n",
    "    ).show()\n",
    "    \n",
    "    # Checkpoint analysis summary\n",
    "    df_checkpoint_check = spark.table(gold_checkpoint_analysis)\n",
    "    print(f\"\\ngold_checkpoint_analysis: {df_checkpoint_check.count()} maps analyzed\")\n",
    "    df_checkpoint_check.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running summary report: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
