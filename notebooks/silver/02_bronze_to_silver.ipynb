{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer: Transform Bronze to Silver Delta Tables\n",
    "\n",
    "## Purpose\n",
    "This notebook transforms raw Bronze data into cleaned, normalized Silver layer tables following dimensional modeling principles.\n",
    "\n",
    "## Inputs\n",
    "- **Source**: `bronze_replays_raw` Delta table in `tm2020_bronze` Lakehouse\n",
    "- **Processing**: Read incremental data since last run (watermark-based)\n",
    "\n",
    "## Outputs\n",
    "- **Target Lakehouse**: `tm2020_silver`\n",
    "- **Tables**:\n",
    "  - `silver_replays`: Cleaned replay metadata (fact table)\n",
    "  - `silver_ghost_samples`: Normalized telemetry samples (fact table)\n",
    "  - `silver_maps`: Map dimension table (SCD Type 2)\n",
    "  - `silver_players`: Player dimension table (SCD Type 2)\n",
    "\n",
    "## Data Quality Rules\n",
    "1. **Mandatory fields**: player_login, map_uid, race_time_ms must not be null\n",
    "2. **Deduplication**: Keep latest record per replay_id\n",
    "3. **Type validation**: Ensure numeric fields are valid\n",
    "4. **Range checks**: race_time_ms > 0, speeds within reasonable bounds\n",
    "5. **Standardization**: Normalize player nicknames, map names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session and imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, explode, current_timestamp, coalesce, lit, \n",
    "    row_number, size, monotonically_increasing_id, concat, md5\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Initialize Spark session (pre-configured in Fabric)\n",
    "spark = SparkSession.builder.appName(\"Silver_Transformation\").getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table names\n",
    "# TODO: Ensure both Bronze and Silver Lakehouses are attached to this notebook\n",
    "bronze_table = \"tm2020_bronze.bronze_replays_raw\"\n",
    "silver_replays_table = \"silver_replays\"\n",
    "silver_ghost_samples_table = \"silver_ghost_samples\"\n",
    "silver_maps_table = \"silver_maps\"\n",
    "silver_players_table = \"silver_players\"\n",
    "\n",
    "print(f\"Bronze source: {bronze_table}\")\n",
    "print(f\"Silver tables: {silver_replays_table}, {silver_ghost_samples_table}, {silver_maps_table}, {silver_players_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Bronze layer\n",
    "# TODO: Implement incremental processing using watermarks\n",
    "# For now, read all data - optimize later with delta log processing\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(bronze_table)\n",
    "    print(f\"Records read from Bronze: {df_bronze.count()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    df_bronze.select(\n",
    "        \"replay_id\", \n",
    "        \"ingestion_timestamp\", \n",
    "        \"metadata.player_login\", \n",
    "        \"metadata.map_uid\", \n",
    "        \"metadata.race_time_ms\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Bronze table: {e}\")\n",
    "    print(\"Ensure Bronze ingestion notebook has been run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality validation and cleaning\n",
    "# TODO: Implement comprehensive data quality rules\n",
    "\n",
    "try:\n",
    "    # Add validation flag\n",
    "    df_validated = df_bronze.withColumn(\n",
    "        \"is_valid\",\n",
    "        (\n",
    "            col(\"metadata.player_login\").isNotNull() &\n",
    "            col(\"metadata.map_uid\").isNotNull() &\n",
    "            col(\"metadata.race_time_ms\").isNotNull() &\n",
    "            (col(\"metadata.race_time_ms\") > 0)\n",
    "        ).cast(BooleanType())\n",
    "    )\n",
    "    \n",
    "    # Report validation results\n",
    "    total_records = df_validated.count()\n",
    "    valid_records = df_validated.filter(col(\"is_valid\") == True).count()\n",
    "    invalid_records = total_records - valid_records\n",
    "    \n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Valid records: {valid_records} ({valid_records/total_records*100:.2f}%)\")\n",
    "    print(f\"Invalid records: {invalid_records} ({invalid_records/total_records*100:.2f}%)\")\n",
    "    \n",
    "    # Keep only valid records for Silver layer\n",
    "    df_clean = df_validated.filter(col(\"is_valid\") == True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in validation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Silver replays table\n",
    "# TODO: Implement deduplication and additional transformations\n",
    "\n",
    "try:\n",
    "    df_silver_replays = df_clean.select(\n",
    "        col(\"replay_id\"),\n",
    "        col(\"metadata.player_login\").alias(\"player_login\"),\n",
    "        col(\"metadata.player_nickname\").alias(\"player_nickname\"),\n",
    "        col(\"metadata.map_uid\").alias(\"map_uid\"),\n",
    "        col(\"metadata.race_time_ms\").alias(\"race_time_ms\"),\n",
    "        coalesce(col(\"metadata.num_respawns\"), lit(0)).alias(\"num_respawns\"),\n",
    "        size(col(\"metadata.checkpoints\")).alias(\"num_checkpoints\"),\n",
    "        col(\"metadata.game_version\").alias(\"game_version\"),\n",
    "        col(\"metadata.title_id\").alias(\"title_id\"),\n",
    "        col(\"ingestion_timestamp\").cast(\"date\").alias(\"ingestion_date\"),\n",
    "        col(\"is_valid\")\n",
    "    )\n",
    "    \n",
    "    # Deduplication: Keep latest record per replay_id\n",
    "    window_spec = Window.partitionBy(\"replay_id\").orderBy(col(\"ingestion_date\").desc())\n",
    "    df_silver_replays_dedup = df_silver_replays \\\n",
    "        .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "        .filter(col(\"row_num\") == 1) \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    print(f\"Silver replays records: {df_silver_replays_dedup.count()}\")\n",
    "    df_silver_replays_dedup.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver_replays: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Silver ghost samples table (normalized telemetry)\n",
    "# TODO: Implement ghost samples normalization\n",
    "\n",
    "try:\n",
    "    # Explode ghost_samples array into individual rows\n",
    "    df_silver_ghost_samples = df_clean.select(\n",
    "        col(\"replay_id\"),\n",
    "        explode(col(\"ghost_samples\")).alias(\"sample\")\n",
    "    ).select(\n",
    "        concat(col(\"replay_id\"), lit(\"_\"), col(\"sample.time_ms\")).alias(\"sample_id\"),\n",
    "        col(\"replay_id\"),\n",
    "        col(\"sample.time_ms\").alias(\"time_ms\"),\n",
    "        col(\"sample.position.x\").alias(\"position_x\"),\n",
    "        col(\"sample.position.y\").alias(\"position_y\"),\n",
    "        col(\"sample.position.z\").alias(\"position_z\"),\n",
    "        col(\"sample.velocity.x\").alias(\"velocity_x\"),\n",
    "        col(\"sample.velocity.y\").alias(\"velocity_y\"),\n",
    "        col(\"sample.velocity.z\").alias(\"velocity_z\"),\n",
    "        col(\"sample.speed\").alias(\"speed\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Silver ghost samples records: {df_silver_ghost_samples.count()}\")\n",
    "    df_silver_ghost_samples.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver_ghost_samples: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Maps dimension table\n",
    "# TODO: Implement SCD Type 2 logic for slowly changing dimensions\n",
    "\n",
    "try:\n",
    "    df_silver_maps = df_clean.select(\n",
    "        col(\"metadata.map_uid\").alias(\"map_uid\"),\n",
    "        col(\"metadata.map_name\").alias(\"map_name\"),\n",
    "        col(\"metadata.map_author\").alias(\"map_author\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    ).distinct()\n",
    "    \n",
    "    # Aggregate to get first and last seen timestamps\n",
    "    df_silver_maps_agg = df_silver_maps.groupBy(\"map_uid\", \"map_name\", \"map_author\").agg(\n",
    "        col(\"ingestion_timestamp\").alias(\"first_seen\"),\n",
    "        col(\"ingestion_timestamp\").alias(\"last_seen\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Silver maps records: {df_silver_maps_agg.count()}\")\n",
    "    df_silver_maps_agg.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver_maps: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Players dimension table\n",
    "# TODO: Implement SCD Type 2 logic for slowly changing dimensions\n",
    "\n",
    "try:\n",
    "    df_silver_players = df_clean.select(\n",
    "        col(\"metadata.player_login\").alias(\"player_login\"),\n",
    "        col(\"metadata.player_nickname\").alias(\"player_nickname\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    ).distinct()\n",
    "    \n",
    "    # Aggregate to get first and last seen timestamps\n",
    "    df_silver_players_agg = df_silver_players.groupBy(\"player_login\", \"player_nickname\").agg(\n",
    "        col(\"ingestion_timestamp\").alias(\"first_seen\"),\n",
    "        col(\"ingestion_timestamp\").alias(\"last_seen\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Silver players records: {df_silver_players_agg.count()}\")\n",
    "    df_silver_players_agg.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver_players: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Silver Delta tables\n",
    "# TODO: Implement merge logic for incremental updates\n",
    "# For now using overwrite mode - optimize later with MERGE statements\n",
    "\n",
    "try:\n",
    "    # Write silver_replays\n",
    "    df_silver_replays_dedup.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(silver_replays_table)\n",
    "    print(f\"Successfully wrote to {silver_replays_table}\")\n",
    "    \n",
    "    # Write silver_ghost_samples\n",
    "    df_silver_ghost_samples.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(silver_ghost_samples_table)\n",
    "    print(f\"Successfully wrote to {silver_ghost_samples_table}\")\n",
    "    \n",
    "    # Write silver_maps\n",
    "    df_silver_maps_agg.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(silver_maps_table)\n",
    "    print(f\"Successfully wrote to {silver_maps_table}\")\n",
    "    \n",
    "    # Write silver_players\n",
    "    df_silver_players_agg.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(silver_players_table)\n",
    "    print(f\"Successfully wrote to {silver_players_table}\")\n",
    "    \n",
    "    print(\"\\nAll Silver tables written successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing Silver tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality report for Silver layer\n",
    "# TODO: Implement comprehensive data quality checks\n",
    "\n",
    "try:\n",
    "    print(\"=== Silver Layer Data Quality Report ===\")\n",
    "    \n",
    "    # silver_replays\n",
    "    df_replays_check = spark.table(silver_replays_table)\n",
    "    print(f\"\\nsilver_replays: {df_replays_check.count()} records\")\n",
    "    print(f\"  Unique players: {df_replays_check.select('player_login').distinct().count()}\")\n",
    "    print(f\"  Unique maps: {df_replays_check.select('map_uid').distinct().count()}\")\n",
    "    \n",
    "    # silver_ghost_samples\n",
    "    df_samples_check = spark.table(silver_ghost_samples_table)\n",
    "    print(f\"\\nsilver_ghost_samples: {df_samples_check.count()} records\")\n",
    "    print(f\"  Unique replays: {df_samples_check.select('replay_id').distinct().count()}\")\n",
    "    \n",
    "    # silver_maps\n",
    "    df_maps_check = spark.table(silver_maps_table)\n",
    "    print(f\"\\nsilver_maps: {df_maps_check.count()} records\")\n",
    "    \n",
    "    # silver_players\n",
    "    df_players_check = spark.table(silver_players_table)\n",
    "    print(f\"\\nsilver_players: {df_players_check.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running data quality checks: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
